# Start from the official Airflow 3.0.6 base image
#FROM apache/airflow:3.0.6
FROM apache/airflow:3.0.6-python3.10
# Switch to the root user to install OS-level packages
USER root

# Install OS-level dependencies for the HDFS provider (Kerberos)
# and the Java Development Kit (JDK) for the spark-submit command.
# Use openjdk-17-jdk as openjdk-11-jdk is not available in the base image's repo.
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    libkrb5-dev \
    libsasl2-dev \
    openjdk-17-jdk \
    && apt-get autoremove -yqq --purge && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set the JAVA_HOME environment variable so spark-submit can find it
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# Switch back to the non-privileged airflow user
USER airflow

# Install the requested Python provider packages
# This "bakes" them into the image for faster startups.
RUN pip install --no-cache-dir \
    "apache-airflow-providers-apache-spark" \
    "apache-airflow-providers-apache-hdfs" \
    "apache-airflow-providers-sftp" \
    "apache-airflow-providers-apache-druid"

